mkdir /sys/fs/cgroup/raph1
mkdir ~/bpf
apt-get install libbpf-dev clang linux-tools-aws
apt-get install llvm libelf-dev libpcap-dev gcc-multilib build-essential
(reboot needed if running kernel is not the latest one installed)
clang -O2 -target bpf -c prog1.c -o prog1.o

For map you need btf so add -g
clang -g -O2 -target bpf -c prog1.c -o prog1.o
ln -s /usr/include/x86_64-linux-gnu/asm/ /usr/include/asm

set a process in the given cgroup (like another bash session or even self process for example)
echo $process >> /sys/fs/cgroup/raph1/cgroup.procs

bpftool prog load ./prog1.o /sys/fs/bpf/prog1 type cgroup/dev
if a map is defined in the program
bpftool map pin name raph_map /sys/fs/bpf/map1
-> to export it for other programs to use it


bpftool prog [list] to show all loaded program and get our program info (id, name, tag whatever can be passed in the following command...)
bpftool cgroup attach /sys/fs/cgroup/raph1/ cgroup_device name bpf_prog1

NOTE: if we attempt to load several times the same program then when listing program you see sth like:
80: cgroup_device  name bpf_prog1  tag f32efaded95a7433  gpl
	loaded_at 2023-10-12T08:21:27+0000  uid 0
	xlated 88B  jited 62B  memlock 4096B
86: cgroup_device  name bpf_prog1  tag f32efaded95a7433  gpl
	loaded_at 2023-10-12T08:35:21+0000  uid 0
	xlated 88B  jited 62B  memlock 4096B

-> so everything in common but the id thus if you try to attach detach a program you need to provide the id so as to not be ambiguous
bpftool cgroup attach /sys/fs/cgroup/raph1/ cgroup_device id 80 (for example)

cat /proc/self/cgroup

nvidia-smi
No devices were found

bpftool cgroup detach /sys/fs/cgroup/raph1/ cgroup_device name bpf_prog1
rm /sys/fs/bpf/prog1


// TODO: see how to use a map and interact directly with userland to
// control which devices get allowed and denied

// Should we attach with mode override or multi ? (cgroup attach flags)

Possibility to deny several devices with several multi attach

# bpftool cgroup attach /sys/fs/cgroup/raph1/ cgroup_device name bpf_prog1 multi
# bpftool cgroup attach /sys/fs/cgroup/raph1/ cgroup_device name bpf_prog2 multi

# nvidia-smi
No devices were found
# cat /dev/urandom
cat: /dev/urandom: Operation not permitted


root@ip-172-31-41-106:~/bpf# bpftool map create /sys/fs/bpf/map1 type array key 4 value 4 entries 10 name mapraph
root@ip-172-31-41-106:~/bpf# bpftool map list
25: array  name mapraph  flags 0x0
	key 4B  value 4B  max_entries 10  memlock 4096B
28: array  name pid_iter.rodata  flags 0x480
	key 4B  value 4B  max_entries 1  memlock 4096B
	btf_id 104  frozen
	pids bpftool(20409)
29: array  name libbpf_det_bind  flags 0x0
	key 4B  value 32B  max_entries 1  memlock 4096B


git clone https://github.com/libbpf/libbpf.git
cd libbpf/src
make


# bpftool prog load ./prog4.o /sys/fs/bpf/prog4 type cgroup/dev
# bpftool cgroup attach /sys/fs/cgroup/raph1/ cgroup_device name bpf_prog2 multi
# bpftool map pin name raph_map /sys/fs/bpf/map1
# ./updater
Loading map in userland
major 195 minor 0
# bpftool prog tracelog


# bpftool cgroup list /sys/fs/cgroup/raph1


Questions that remain to be answered:

When the space is created this ends up with a kub deployment + pod creation + containerd.
How to make sure the bpf program gets loaded at the container creation in kube
-> Init container (not ideal) ? -> Directement appliquer le deny au cgroup commun /kubepods.slice/kubepods-burstable.slice/kubepods-burstable-podc958ac65_e247_4056_8fbb_d0a92501c49c.slice/
-> nvidia-containerd runtime wrapper (which is already a containerd wrapper) ? -> yes could be a good solution
-> nvidia-containerd amend
